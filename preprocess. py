import os
import glob
import argparse
import subprocess
import sys
import math
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from tqdm import tqdm
from PIL import Image

# --- CONFIG ---
BATCH_SIZE = 256
NUM_WORKERS = 4  # Safe to use workers now because processes are isolated

class PatchDataset(Dataset):
    def __init__(self, file_list, transform=None):
        self.files = file_list
        self.transform = transform
    def __len__(self): return len(self.files)
    def __getitem__(self, idx):
        path = self.files[idx]
        try:
            with Image.open(path) as img:
                return self.transform(img.convert('RGB')), path
        except:
            return torch.zeros(3, 224, 224), path

def get_backbone(weights_path, device):
    # Load model on specific device to avoid memory overhead
    model = models.resnet34(weights=None)
    
    checkpoint = torch.load(weights_path, map_location='cpu')
    state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
    
    if 'fc.weight' in new_state_dict:
        num_classes = new_state_dict['fc.weight'].shape[0]
        model.fc = nn.Linear(512, num_classes)
        
    try:
        model.load_state_dict(new_state_dict)
    except:
        model.load_state_dict(new_state_dict, strict=False)

    model.fc = nn.Identity()
    model = model.to(device)
    model.eval()
    return model

def run_worker(args):
    """Worker logic: Runs on a single GPU for a subset of classes"""
    gpu_id = args.gpu_id
    device = torch.device(f"cuda:{gpu_id}")
    
    print(f"[Worker {gpu_id}] Initializing on {device}...")
    
    # 1. Load Model
    model = get_backbone(args.weights_path, device)
    
    # 2. Setup Transform
    tfm = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # 3. Get my slice of classes
    all_classes = sorted([d for d in os.listdir(args.data_dir) if os.path.isdir(os.path.join(args.data_dir, d))])
    
    # Calculate chunk
    chunk_size = math.ceil(len(all_classes) / args.total_shards)
    start_idx = args.shard_id * chunk_size
    end_idx = min(start_idx + chunk_size, len(all_classes))
    my_classes = all_classes[start_idx:end_idx]
    
    print(f"[Worker {gpu_id}] Processing {len(my_classes)} classes ({start_idx} to {end_idx})...")
    
    # 4. Processing Loop
    with torch.no_grad():
        for cls in tqdm(my_classes, desc=f"GPU {gpu_id}", position=gpu_id):
            src_folder = os.path.join(args.data_dir, cls)
            dst_folder = os.path.join(args.output_dir, cls)
            os.makedirs(dst_folder, exist_ok=True)
            
            # Find images
            all_imgs = glob.glob(os.path.join(src_folder, "*.jpg"))
            if not all_imgs: continue

            # Group by Prescription
            rx_map = {}
            for p in all_imgs:
                fname = os.path.basename(p)
                rx_id = fname.split('_patch_')[0] if "_patch_" in fname else fname.split('.')[0]
                if rx_id not in rx_map: rx_map[rx_id] = []
                rx_map[rx_id].append(p)
            
            # Extract
            for rx_id, paths in rx_map.items():
                save_path = os.path.join(dst_folder, rx_id + ".pt")
                if os.path.exists(save_path): continue

                ds = PatchDataset(paths, transform=tfm)
                loader = DataLoader(
                    ds, 
                    batch_size=BATCH_SIZE, 
                    num_workers=NUM_WORKERS, 
                    pin_memory=True
                )
                
                features_list = []
                # Simple mixed precision
                with torch.cuda.amp.autocast():
                    for imgs, _ in loader:
                        imgs = imgs.to(device)
                        feats = model(imgs)
                        features_list.append(feats.cpu())
                
                if len(features_list) > 0:
                    full_tensor = torch.cat(features_list, dim=0)
                    torch.save(full_tensor, save_path)

    print(f"[Worker {gpu_id}] Finished!")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--weights_path", type=str, required=True)
    # Internal arguments for workers
    parser.add_argument("--launcher", action="store_true", help="Launch parallel workers")
    parser.add_argument("--shard_id", type=int, default=0)
    parser.add_argument("--total_shards", type=int, default=1)
    parser.add_argument("--gpu_id", type=int, default=0)
    args = parser.parse_args()

    if args.launcher:
        # --- LAUNCHER MODE ---
        num_gpus = torch.cuda.device_count()
        print(f"--> Detected {num_gpus} GPUs. Launching {num_gpus} parallel processes...")
        
        processes = []
        for i in range(num_gpus):
            # Construct command to call self as a worker
            cmd = [
                sys.executable, __file__,
                "--data_dir", args.data_dir,
                "--output_dir", args.output_dir,
                "--weights_path", args.weights_path,
                "--shard_id", str(i),
                "--total_shards", str(num_gpus),
                "--gpu_id", str(i)
            ]
            # Launch without waiting
            p = subprocess.Popen(cmd)
            processes.append(p)
            
        print("--> All workers launched. Waiting for completion...")
        for p in processes:
            p.wait()
        print("--> All Done!")
        
    else:
        # --- WORKER MODE ---
        run_worker(args)

if __name__ == "__main__":
    main()
